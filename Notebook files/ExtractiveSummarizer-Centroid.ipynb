{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing and downloading necessary libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport os\nimport random\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-22T04:40:49.594178Z","iopub.execute_input":"2022-04-22T04:40:49.594661Z","iopub.status.idle":"2022-04-22T04:40:49.621256Z","shell.execute_reply.started":"2022-04-22T04:40:49.594579Z","shell.execute_reply":"2022-04-22T04:40:49.620470Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom scipy.spatial.distance import cosine","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.517171Z","iopub.execute_input":"2022-04-19T14:27:35.517493Z","iopub.status.idle":"2022-04-19T14:27:35.524028Z","shell.execute_reply.started":"2022-04-19T14:27:35.517444Z","shell.execute_reply":"2022-04-19T14:27:35.522705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.537173Z","iopub.execute_input":"2022-04-19T14:27:35.537611Z","iopub.status.idle":"2022-04-19T14:27:35.547181Z","shell.execute_reply.started":"2022-04-19T14:27:35.537573Z","shell.execute_reply":"2022-04-19T14:27:35.546499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing a dataset containing gold summaries (for reference)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/bbc-articles/BBCarticles_csv.csv\", encoding = \"unicode_escape\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.57069Z","iopub.execute_input":"2022-04-19T14:27:35.571018Z","iopub.status.idle":"2022-04-19T14:27:35.684799Z","shell.execute_reply.started":"2022-04-19T14:27:35.570984Z","shell.execute_reply":"2022-04-19T14:27:35.68399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preprocessing the dataset","metadata":{}},{"cell_type":"code","source":"df = df.dropna().reset_index()\ndf['Text'] = df['Text'].apply(lambda x: x.replace('\\n',' '))\ndf['Summary'] = df['Summary'].apply(lambda x: x.replace('\\n',' '))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.686237Z","iopub.execute_input":"2022-04-19T14:27:35.687132Z","iopub.status.idle":"2022-04-19T14:27:35.711737Z","shell.execute_reply.started":"2022-04-19T14:27:35.687096Z","shell.execute_reply":"2022-04-19T14:27:35.710677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up inputs","metadata":{}},{"cell_type":"code","source":"rand = random.randint(0,df.shape[0])\nprint(rand)\nsample_text = df.iloc[rand,2]\ngold_summary = df.iloc[rand,1]\nprint(\"\\nText: \", sample_text)\nprint(\"\\nGold Summary: \", gold_summary)\n\n# sample_text= \"Tanjiro Kamado is a kind-hearted and intelligent boy who lives with his family in the mountains. He became his family's breadwinner after his father's death, making trips to the nearby village to sell charcoal. Everything changed when he came home one day to discover that his family was attacked and slaughtered by a demon. Tanjiro and his sister Nezuko were the sole survivors of the incident, with Nezuko being transformed into a demon, but still surprisingly showing signs of human emotion and thought. After an encounter with Giyū Tomioka, a demon slayer, Tanjiro is recruited by Giyū and sent to his retired master Sakonji Urokodaki for training to also become a demon slayer, beginning his quest to help his sister turn into human again and avenge the death of his family. After two years of strenuous training, Tanjiro takes part in a formidable exam and is one of the few survivors to pass, officially making him a member of the Demon Slayer Corps. He begins his work of hunting down and slaying demons alongside Nezuko, who has been hypnotized to bring no harm to humans and who occasionally helps him in battle. One of Tanjiro's assignments brings him to Asakusa where he encounters Muzan Kibutsuji, the progenitor of all demons and the one who murdered his family. He also meets Tamayo, a demon who is free of Muzan's control. Tamayo allies with Tanjiro and begins to develop a cure for Nezuko, though it will require Tanjiro to supply her with blood from the Twelve Kizuki, the most powerful demons under Muzan's command.\"","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.71292Z","iopub.execute_input":"2022-04-19T14:27:35.713131Z","iopub.status.idle":"2022-04-19T14:27:35.721002Z","shell.execute_reply.started":"2022-04-19T14:27:35.713105Z","shell.execute_reply":"2022-04-19T14:27:35.720265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Preprocessing Input Text","metadata":{}},{"cell_type":"code","source":"def preprocess_input_text(text):\n    stop_words = set(stopwords.words('english'))\n    sentences = sent_tokenize(text)\n    preprocessed_sentences = []\n    for sent in sentences:\n        words = word_tokenize(sent)\n        words = [w for w in words if w not in string.punctuation]\n        words = [w for w in words if not w.lower() in stop_words]\n        words = [w.lower() for w in words]\n        #words = [w.replace('\"', \"'\") for w in words]\n        preprocessed_sentences.append(\" \".join(words))\n    return preprocessed_sentences","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.723109Z","iopub.execute_input":"2022-04-19T14:27:35.723356Z","iopub.status.idle":"2022-04-19T14:27:35.766776Z","shell.execute_reply.started":"2022-04-19T14:27:35.723326Z","shell.execute_reply":"2022-04-19T14:27:35.765844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_sentences = preprocess_input_text(sample_text)\ntokenized_words = []\nfor sent in preprocessed_sentences:\n    tokenized_words.append(word_tokenize(sent))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.768466Z","iopub.execute_input":"2022-04-19T14:27:35.769304Z","iopub.status.idle":"2022-04-19T14:27:35.793544Z","shell.execute_reply.started":"2022-04-19T14:27:35.769254Z","shell.execute_reply":"2022-04-19T14:27:35.792491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_tokenize(text):\n    sents = sent_tokenize(text)\n    sents_filtered = []\n    for s in sents:\n        sents_filtered.append(s)\n    return sents_filtered","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.794648Z","iopub.execute_input":"2022-04-19T14:27:35.794881Z","iopub.status.idle":"2022-04-19T14:27:35.807532Z","shell.execute_reply.started":"2022-04-19T14:27:35.794846Z","shell.execute_reply":"2022-04-19T14:27:35.806726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Embedding Model","metadata":{}},{"cell_type":"code","source":"embedding_model = Word2Vec(tokenized_words, min_count=1, sg = 1, epochs = 1000)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:35.809257Z","iopub.execute_input":"2022-04-19T14:27:35.809558Z","iopub.status.idle":"2022-04-19T14:27:37.866026Z","shell.execute_reply.started":"2022-04-19T14:27:35.809527Z","shell.execute_reply":"2022-04-19T14:27:37.865145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculate TF-IDF Scores","metadata":{}},{"cell_type":"code","source":"def calculate_tf_idf(sentences):\n    vectorizer = CountVectorizer()\n    sent_word_matrix = vectorizer.fit_transform(sentences)\n    transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=False)\n    tfidf = transformer.fit_transform(sent_word_matrix)\n    tfidf = tfidf.toarray()\n    centroid_vector = tfidf.sum(0)\n    centroid_vector = np.divide(centroid_vector, centroid_vector.max())\n    feature_names = vectorizer.get_feature_names_out()\n    centroid_limit = 0.3\n    relevant_vector_indices = np.where(centroid_vector > centroid_limit)[0]\n\n    word_list = list(np.array(feature_names)[relevant_vector_indices])\n    return word_list","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.867763Z","iopub.execute_input":"2022-04-19T14:27:37.868056Z","iopub.status.idle":"2022-04-19T14:27:37.876111Z","shell.execute_reply.started":"2022-04-19T14:27:37.868014Z","shell.execute_reply":"2022-04-19T14:27:37.87498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Populating word vectors (with word embeddings)","metadata":{}},{"cell_type":"code","source":"def word_vectors_cache(sentences, embedding_model):\n    word_vectors = dict()\n    for sent in sentences:\n        words = word_tokenize(sent)\n        for w in words:\n            word_vectors.update({w: embedding_model.wv[w]})\n    return word_vectors","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.877353Z","iopub.execute_input":"2022-04-19T14:27:37.877984Z","iopub.status.idle":"2022-04-19T14:27:37.890003Z","shell.execute_reply.started":"2022-04-19T14:27:37.877948Z","shell.execute_reply":"2022-04-19T14:27:37.88924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence embedding representation with sum of word vectors","metadata":{}},{"cell_type":"code","source":"def build_embedding_representation(words, word_vectors, embedding_model):\n    embedding_representation = np.zeros(embedding_model.vector_size, dtype=\"float32\")\n    word_vectors_keys = set(word_vectors.keys())\n    count = 0\n    for w in words:\n        if w in word_vectors_keys:\n            embedding_representation = embedding_representation + word_vectors[w]\n            count += 1\n    if count != 0:\n        embedding_representation = np.divide(embedding_representation, count)\n    return embedding_representation","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.891967Z","iopub.execute_input":"2022-04-19T14:27:37.892704Z","iopub.status.idle":"2022-04-19T14:27:37.905775Z","shell.execute_reply.started":"2022-04-19T14:27:37.892667Z","shell.execute_reply":"2022-04-19T14:27:37.904302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cosine Similarity","metadata":{}},{"cell_type":"code","source":"def calculate_cosine_similarity(vector1, vector2):\n    score = 0.0\n    if np.count_nonzero(vector1) != 0 and np.count_nonzero(vector2) != 0:\n        score = ((1 - cosine(vector1, vector2)) + 1) / 2\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.90709Z","iopub.execute_input":"2022-04-19T14:27:37.907503Z","iopub.status.idle":"2022-04-19T14:27:37.925985Z","shell.execute_reply.started":"2022-04-19T14:27:37.907454Z","shell.execute_reply":"2022-04-19T14:27:37.924973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Extractive Summary","metadata":{}},{"cell_type":"code","source":"def generate_summary(text, embedding_model):\n    raw_sentences = sentence_tokenize(text)\n    clean_sentences = preprocess_input_text(text)\n    centroid_words = calculate_tf_idf(clean_sentences)\n    word_vectors = word_vectors_cache(clean_sentences, embedding_model)\n    centroid_vector = build_embedding_representation(centroid_words, word_vectors, embedding_model)\n    sentences_scores = []\n    for i in range(len(clean_sentences)):\n        scores = []\n        words = clean_sentences[i].split()\n        sentence_vector = build_embedding_representation(words, word_vectors, embedding_model)\n        score = calculate_cosine_similarity(sentence_vector, centroid_vector)\n        sentences_scores.append((i, raw_sentences[i], score, sentence_vector))\n    sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)\n    return sentence_scores_sort","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.927091Z","iopub.execute_input":"2022-04-19T14:27:37.927639Z","iopub.status.idle":"2022-04-19T14:27:37.941278Z","shell.execute_reply.started":"2022-04-19T14:27:37.927562Z","shell.execute_reply":"2022-04-19T14:27:37.940288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing redundancy","metadata":{}},{"cell_type":"code","source":"def remove_redundancy(sentence_scores, limit,limit_type ):\n    count = 0\n    sentences_summary = []\n    for s in sentence_scores:\n        if count>limit:\n            break\n        include_flag = True\n        for ps in sentences_summary:\n            sim = calculate_cosine_similarity(s[3], ps[3])\n            if sim > 0.95:\n                include_flag = False\n        if include_flag:\n            sentences_summary.append(s)\n            if limit_type == \"word\":\n                count += len(s[1].split())\n            elif limit_type == \"sentence\":\n                count += 1\n\n        sentences_summary = sorted(sentences_summary, key=lambda el: el[0], reverse=False)\n\n    summary = \"\".join([s[1] for s in sentences_summary])\n    return summary","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.942802Z","iopub.execute_input":"2022-04-19T14:27:37.943178Z","iopub.status.idle":"2022-04-19T14:27:37.954087Z","shell.execute_reply.started":"2022-04-19T14:27:37.943055Z","shell.execute_reply":"2022-04-19T14:27:37.953085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get summary","metadata":{}},{"cell_type":"code","source":"print(\"Number of sentences: \", len(preprocessed_sentences))\nwords = sample_text.split()\nword_count = len(words)\nprint(\"Word count: \", word_count)\nword_limit = 100\nsentence_limit = 5","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.955521Z","iopub.execute_input":"2022-04-19T14:27:37.955766Z","iopub.status.idle":"2022-04-19T14:27:37.974678Z","shell.execute_reply.started":"2022-04-19T14:27:37.955737Z","shell.execute_reply":"2022-04-19T14:27:37.973633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_scores = generate_summary(sample_text, embedding_model)\nextractive_summary = remove_redundancy(sentence_scores, sentence_limit, \"sentence\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:37.97756Z","iopub.execute_input":"2022-04-19T14:27:37.977987Z","iopub.status.idle":"2022-04-19T14:27:38.000879Z","shell.execute_reply.started":"2022-04-19T14:27:37.977939Z","shell.execute_reply":"2022-04-19T14:27:37.999543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Print Summaries","metadata":{}},{"cell_type":"code","source":"print(\"\\nText:  \", sample_text)\nprint(\"\\nGold Summary: \", gold_summary)\nprint(\"\\nExtractive Summary: \", extractive_summary)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:27:38.004839Z","iopub.execute_input":"2022-04-19T14:27:38.005254Z","iopub.status.idle":"2022-04-19T14:27:38.013114Z","shell.execute_reply.started":"2022-04-19T14:27:38.005219Z","shell.execute_reply":"2022-04-19T14:27:38.012172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}