{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ExtractiveSummary-TFIDF.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tAovm3y4u_lc"},"source":["Extractive Summary (Using TF-IDF and Cosine Distance)\n","\n","Steps: Input document -> Finding most important words from the document -> Finding sentence scores on the basis of important words -> Choosing the most important sentences on the basis of scores obtained/Finding similarity between sentences and ranking them based on pairwise cosine similarity (combining with the approach used in Cosine Distance summary extraction)-> Merging the chosen sentences to form a summary.\n","\n","Formulas Used:\n","TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n","\n","IDF(w) = log_e(Total number of documents / Number of documents with term w in it)\n","\n","TFIDF(w) = TF(w) * IDF(w)\n","\n","Cos(x, y) = x . y / ||x|| * ||y||"]},{"cell_type":"markdown","metadata":{"id":"lM3cWxvrwgW7"},"source":["\n","\n","> Importing necessary libraries\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tOyIjpoWwofr","executionInfo":{"status":"ok","timestamp":1634812653460,"user_tz":-330,"elapsed":378,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}},"outputId":"bd41081f-f15a-4aeb-a4e5-c2c64f363695"},"source":["import nltk\n","import os\n","import re\n","import math\n","import operator\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize,word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.cluster.util import cosine_distance\n","Stopwords = set(stopwords.words('english'))\n","wordlemmatizer = WordNetLemmatizer()"],"execution_count":142,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ibLWnlzHyF1W","executionInfo":{"status":"ok","timestamp":1634812653886,"user_tz":-330,"elapsed":29,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}},"outputId":"596a2a49-5593-4959-93f1-557649b066e3"},"source":["from google.colab import drive\n","drive.mount('/content/drive' )"],"execution_count":143,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"FhKJzeDXxSKx"},"source":["\n","\n","> Text preprocessing\n","\n","The pre-processing steps applied in this algorithm include removing special characters, digits, one-letter words and stop words from the text .\n"]},{"cell_type":"markdown","metadata":{"id":"gNnvEKBZ0bPM"},"source":["Function to remove special characters from the text"]},{"cell_type":"code","metadata":{"id":"ON96FZQY0fQ1","executionInfo":{"status":"ok","timestamp":1634812654757,"user_tz":-330,"elapsed":875,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def removeSpecialCharacters(text):\n","    regex = r'[^a-zA-Z0-9\\s]'\n","    text = re.sub(regex,'',text)\n","    return text"],"execution_count":144,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eFpmSQgJ1PEe"},"source":["Function to tokenize sentences"]},{"cell_type":"code","metadata":{"id":"vhtEePYRxm0e","executionInfo":{"status":"ok","timestamp":1634812654758,"user_tz":-330,"elapsed":35,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def tokenizeSentences():\n","  file = '/content/drive/MyDrive/LY Project/inputtext.txt'\n","  file = open(file , 'r')\n","  text = file.read()\n","  #Tokenize Sentences\n","  tokenized_sentence = sent_tokenize(text)\n","  #Removing Special Characters\n","  text = removeSpecialCharacters(str(text))\n","  text = re.sub(r'\\d+', '', text)\n","  #Tokenize Words\n","  tokenized_words = word_tokenize(text)\n","  #Remove Stop Words\n","  tokenized_words_without_stopwords = [word for word in tokenized_words if word not in Stopwords]\n","  #Remove Single Letter words\n","  tokenized_words_without_stopwords = [word for word in tokenized_words_without_stopwords if len(word) > 1]\n","  #Convert all tokenized words into lower case to remove ambiguity\n","  tokenized_words_without_stopwords = [word.lower() for word in tokenized_words_without_stopwords]\n","  return tokenized_words_without_stopwords, tokenized_sentence"],"execution_count":145,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_qKK0a82L_v"},"source":["Function to calculate the frequency of each word in the document"]},{"cell_type":"code","metadata":{"id":"l5sAjjNg2Rpa","executionInfo":{"status":"ok","timestamp":1634812654759,"user_tz":-330,"elapsed":35,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def calculateWordFrequency(words_list):\n","    words_list = [word.lower() for word in words_list]\n","    frequency = {}\n","    unique_words = []\n","    #Find all unique words\n","    for word in words_list:\n","       if word not in unique_words:\n","          unique_words.append(word)\n","    for word in unique_words:\n","          frequency[word] = words_list.count(word)\n","    return frequency"],"execution_count":146,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ATpvWBi3ksO"},"source":["Function to calculate sentence scores (Using TF-IDF)\n","\n","Utility functions for sentence scoring: \n","\n","1. POS tagging function (Part of Speech tagging): Using nltk library to pos tag all the words in the text and returns only the nouns and verbs from the text."]},{"cell_type":"code","metadata":{"id":"MWRrUw4t3oGP","executionInfo":{"status":"ok","timestamp":1634812654759,"user_tz":-330,"elapsed":34,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def posTagging(text):\n","    pos_tag = nltk.pos_tag(text.split())\n","    pos_tagged_noun_verb = []\n","    for word,tag in pos_tag:\n","        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n","            pos_tagged_noun_verb.append(word)\n","    return pos_tagged_noun_verb"],"execution_count":147,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ase2QHmS7bDc"},"source":["2. Stemming:  extract the base form of the words by removing affixes"]},{"cell_type":"code","metadata":{"id":"7e42UNmv7sKc","executionInfo":{"status":"ok","timestamp":1634812654759,"user_tz":-330,"elapsed":33,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def stemWords(words):\n","    stemmed_words = []\n","    for word in words:\n","       stemmed_words.append(stemmer.stem(word))\n","    return stemmed_words"],"execution_count":148,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Glc5G8Ka6FhQ"},"source":["3. Lemmatization: grouping together the different inflected forms of a word so they can be analyzed as a single item in context to the word"]},{"cell_type":"code","metadata":{"id":"X50eiYBn7vF5","executionInfo":{"status":"ok","timestamp":1634812654760,"user_tz":-330,"elapsed":33,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def lemmatizeWords(words):\n","    lemmatized_words = []\n","    for word in words:\n","       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n","    return lemmatized_words"],"execution_count":149,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxkK8KZA4xYz"},"source":["4. TF score: It is calculated as the number of times the word appears in the sentence upon the total number of words in the sentence."]},{"cell_type":"code","metadata":{"id":"hIPkHwHL484H","executionInfo":{"status":"ok","timestamp":1634812654760,"user_tz":-330,"elapsed":33,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def tfScore(word,sentence):\n","    freq_sum = 0\n","    word_frequency_in_sentence = 0\n","    len_sentence = len(sentence)\n","    for word_in_sentence in sentence.split():\n","        if word == word_in_sentence:\n","            word_frequency_in_sentence = word_frequency_in_sentence + 1\n","    tf =  word_frequency_in_sentence/ len_sentence\n","    return tf"],"execution_count":150,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZTNhwqG5GU4"},"source":["5. IDF score: This function finds the idf score of the word, by dividing the total number of sentences by number of sentences containing the word and then taking a log10 of that value."]},{"cell_type":"code","metadata":{"id":"iQP3ekWT5LCQ","executionInfo":{"status":"ok","timestamp":1634812654761,"user_tz":-330,"elapsed":33,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def idfScore(no_of_sentences,word,sentences):\n","    no_of_sentence_containing_word = 0\n","    for sentence in sentences:\n","        sentence = removeSpecialCharacters(str(sentence))\n","        sentence = re.sub(r'\\d+', '', sentence)\n","        sentence = sentence.split()\n","        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n","        sentence = [word.lower() for word in sentence]\n","        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n","        if word in sentence:\n","            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n","    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n","    return idf"],"execution_count":151,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4d17hTh5U9s"},"source":["6. TF-IDF score: multiplies tf and idf values"]},{"cell_type":"code","metadata":{"id":"mDZRv9vL5c3v","executionInfo":{"status":"ok","timestamp":1634812654761,"user_tz":-330,"elapsed":33,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def calculateTfIdfScore(tf,idf):\n","   return tf*idf"],"execution_count":152,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMU11TAqVCsm"},"source":["TF-IDF score for all tokenized words"]},{"cell_type":"code","metadata":{"id":"mWfKuY11VHXu","executionInfo":{"status":"ok","timestamp":1634812654761,"user_tz":-330,"elapsed":33,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def tfidfAllWords(dict_freq,word,sentences,sentence):\n","    word_tfidf = []\n","    tf = tfScore(word,sentence)\n","    idf = idfScore(len(sentences),word,sentences)\n","    tf_idf = calculateTfIdfScore(tf,idf)\n","    return tf_idf"],"execution_count":153,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hCyiq69c87UQ"},"source":["Sentence Ranking using TF-IDF scores (Sum of TF-IDf scores of all words in the sentence)"]},{"cell_type":"code","metadata":{"id":"YpRlR4Nz9bSF","executionInfo":{"status":"ok","timestamp":1634812654762,"user_tz":-330,"elapsed":32,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["def calculateSentenceScore(sentence,frequency,sentences):\n","     sentence_score = 0\n","     sentence = removeSpecialCharacters(str(sentence))\n","     sentence = re.sub(r'\\d+', '', sentence)\n","     pos_tagged_sentence = []\n","     no_of_sentences = len(sentences)\n","     pos_tagged_sentence = posTagging(sentence)\n","     for word in pos_tagged_sentence:\n","         if word.lower() not in Stopwords and word not in Stopwords and len(word)>1:\n","             word = word.lower() \n","             word = wordlemmatizer.lemmatize(word)\n","             sentence_score = sentence_score + tfidfAllWords(frequency,word,sentences,sentence)\n","     return sentence_score"],"execution_count":154,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"21ZarAR9ClCA"},"source":["Calculating Cosine Similarity"]},{"cell_type":"code","metadata":{"id":"tYoVueGFhDMf","executionInfo":{"status":"ok","timestamp":1634812654762,"user_tz":-330,"elapsed":31,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["#Calculate cosine similarity"],"execution_count":155,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gkUXaBn2_maW"},"source":["Calling word tokenization function, performing lemmatization on tokenized words and calculating word frequency"]},{"cell_type":"code","metadata":{"id":"McZYppOM_2w2","executionInfo":{"status":"ok","timestamp":1634812654763,"user_tz":-330,"elapsed":31,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["tokenized_words, tokenized_sentences = tokenizeSentences()\n","tokenized_words = lemmatizeWords(tokenized_words)\n","word_frequency = calculateWordFrequency(tokenized_words)"],"execution_count":156,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v77FvmSV-Ei5"},"source":["Taking input from the user: Percentage of retained context from the original text in the summary\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuHnvk_1_cXG","executionInfo":{"status":"ok","timestamp":1634812660197,"user_tz":-330,"elapsed":5465,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}},"outputId":"8a3cbe56-4344-4210-a309-87e36b34891f"},"source":["retention_percentage = int(input('Percentage of information to retain (in percent):'))\n","no_of_sentences = int((retention_percentage * len(tokenized_sentences))/100)\n","print(\"Number of sentences in the summary: \", no_of_sentences, \"\\nTotal number of sentences: \", len(tokenized_sentences))"],"execution_count":157,"outputs":[{"output_type":"stream","name":"stdout","text":["Percentage of information to retain (in percent):40\n","Number of sentences in the summary:  8 \n","Total number of sentences:  22\n"]}]},{"cell_type":"markdown","metadata":{"id":"nxHWHi4fVq5-"},"source":["Generate summary by sorting the sentences based on the sum of tf-idf scores of all the words in the sentence."]},{"cell_type":"code","metadata":{"id":"5LkliIFDVvoI","executionInfo":{"status":"ok","timestamp":1634812661090,"user_tz":-330,"elapsed":897,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}}},"source":["c = 1\n","sentence_scores = {}\n","for sent in tokenized_sentences:\n","    sentence_importance = calculateSentenceScore(sent,word_frequency,tokenized_sentences)\n","    sentence_scores[c] = sentence_importance\n","    c = c+1\n","sentence_scores = sorted(sentence_scores.items(), key=operator.itemgetter(1),reverse=True)\n","count = 0\n","summary = []\n","sentence_no = []\n","for word in sentence_scores:\n","    if count < no_of_sentences:\n","        sentence_no.append(word[0])\n","        count = count+1\n","    else:\n","      break\n","sentence_no.sort()\n","count = 1\n","for sentence in tokenized_sentences:\n","    if count in sentence_no:\n","       summary.append(sentence)\n","    count = count+1\n","summary = \" \".join(summary)"],"execution_count":158,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W1u0jeOLXq73"},"source":["Print Summary"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DKf6khjVwww","executionInfo":{"status":"ok","timestamp":1634812661091,"user_tz":-330,"elapsed":9,"user":{"displayName":"ANINA PILLAI","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07839762009351267052"}},"outputId":"21bacfd3-91c3-4b37-8a3a-e4c0c6f3ba61"},"source":["print(\"Summary:\")\n","lines = summary.split('.')\n","for line in lines:\n","  print(line)\n"],"execution_count":159,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary:\n","Winston Churchill was an inspirational statesman, writer, orator and leader who led Britain to victory in the Second World War\n"," Although achieving poor grades at school, his early fascination with militarism saw him join the Royal Cavalry in 1895\n"," Heavily criticised for this error, he resigned from this position and travelled to the Western Front to fight himself\n"," Some of his most memorable speeches were given in this period, and are credited with stimulating British morale during periods of great hardship\n"," In his 1946 speech in the USA, the instinctive pro-American famously declared that “an iron curtain has descended across the Continent”, and warned of the continued danger from a powerful Soviet Russia\n"," Ageing and increasingly unwell, he often conducted business from his bedside, and while his powerful personality and oratory ability endured, the Prime Minister’s leadership was less decisive than during the war\n"," His later attempts at decreasing the developing Cold War through personal diplomacy failed to produce significant results, and poor health forced him to resign in 1955, making way for his Foreign Secretary and Deputy Prime Minister, Anthony Eden\n"," Churchill died in 1965, and was honoured with a state funeral\n","\n"]}]}]}